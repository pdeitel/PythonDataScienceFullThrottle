{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2024 by Pearson Education, Inc. All Rights Reserved. The content in this notebook is based on the book [**Python for Programmers**](https://amzn.to/2VvdnxE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- CSS settings for this notbook -->\n",
    "<style>\n",
    "    h1 {color:#BB0000}\n",
    "    h2 {color:purple}\n",
    "    h3 {color:#0099ff}\n",
    "    hr {    \n",
    "        border: 0;\n",
    "        height: 3px;\n",
    "        background: #333;\n",
    "        background-image: linear-gradient(to right, #ccc, black, #ccc);\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enable high-res images in notebook \n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.1 Introduction\n",
    "* Natural language communication examples    \n",
    "    * **Conversations** between people \n",
    "    * Reading/writing **text messages**\n",
    "    * Learning a **foreign language**  \n",
    "    * Using a **smartphone** to read menus in other languages\n",
    "* NLP is performed on **text collections** (**corpora**, plural of **corpus**)\n",
    "    * **Social media posts** (Tweets, Facebook posts, etc.)\n",
    "    * Documents, books, news articles, movie reviews\n",
    "    * And more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning and Deep Learning Natural Language Applications\n",
    "* **Sentiment analysis**\n",
    "* **Speech synthesis** (text-to-speech)\n",
    "* **Speech recognition** (speech-to-text)\n",
    "* **Inter-language text-to-text and speech-to-speech translation**\n",
    "* **Automatic closed captioning**\n",
    "* **Bots answering natural language questions** \n",
    "* **Text summarization**\n",
    "* **Text simplification**\n",
    "* **Recommender systems** (“if you liked this movie, you might also like…”)\n",
    "* **Classifying articles by categories**\n",
    "* **Topic modeling**—finding the **topics** discussed in documents\n",
    "* **Speech to sign language and vice versa**—to enable a conversation with a hearing-impaired person\n",
    "* **Lip reader technology**—for people who can’t speak, convert lip movement to text or speech to enable conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.2 [TextBlob](https://textblob.readthedocs.io/)\n",
    "### Install **TextBlob**\n",
    "* `conda install -c conda-forge textblob`\n",
    "* Next download **NLTK corpora** required by Textblob\n",
    "> `ipython -m textblob.download_corpora`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.1 Create a TextBlob—The fundamental Class for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = 'Yesterday was a beautiful day. Tomorrow looks like bad weather.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.2 Tokenizing Text into Sentences and Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob.sentences  # returns list of Sentence objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob.words  # returns a WordList (subclass of list) of Words; punctuation removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.3 Parts-of-Speech (POS) Tagging\n",
    "* `nltk` [**parts-of-speech** tags](https://www.guru99.com/pos-tagging-chunking-nltk.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob.tags  # list of (word, part-of-speech-tag) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<!--\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure the NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Text to analyze\n",
    "text = \"Yesterday was a beautiful day. Tomorrow looks like bad weather.\"\n",
    "\n",
    "# Tokenize and tag parts of speech\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "pos_tags\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.4 Extracting Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob.noun_phrases  # WordList of Word objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.5 Sentiment Analysis on `TextBlob`s and `Sentence`s\n",
    "* **`polarity`** is the **sentiment** — from **`-1.0` (negative)** to **`1.0` (positive)** &mdash; **`0.0`** is **neutral**\n",
    "* **`subjectivity`** &mdash; **0.0 (objective)** to **1.0 (subjective)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob.sentiment  # Sentiment object positive/negative and objective/subjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sentence in blob.sentences:\n",
    "    print(f'{sentence}\\nsentiment: {sentence.sentiment}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 11.2.7 Language Detection and Translation (1 of 3)\n",
    "* **Google Translate**, **Microsoft Bing Translator** and others can translate among scores of languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** TextBlob translate method is now deprecated. Instead, you can install https://github.com/DeepLcom/deepl-python\n",
    "\n",
    "> `pip install --upgrade deepl`\n",
    "\n",
    "* You'll need an API key\n",
    "* Free one allows 500,000 characters/month\n",
    "* To get a key:\n",
    "> * Go to https://www.deepl.com/pro#developer\n",
    "> * Click **Sign up for free**\n",
    "> * Under **DeepL API Free** click **Sign up for free**\n",
    "> * Specify an email/password and click **Continue**\n",
    "> * Fill in the form and provide a credit card – required to “fraudulent multiple registrations”, then click **Continue**\n",
    "> * Read the terms and, if you agree, click **Sign up for free**\n",
    "> * Click the **Account Management** link on the thank you page\n",
    "> * Click the **Account** tab and scroll to **Authentication Key for DeepL API**\n",
    "> * Copy your key then in the **ch11** folder create a **`keys.py`** file containing\n",
    ">> `deepL_key = 'your key here'`\n",
    "> * Be sure to replace the contents of the preceding string with your DeepL key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import deepl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translator = deepl.Translator(keys.deepL_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--**NOTE:** TextBlob translate method is now deprecated. Instead, you can install https://deep-translator.readthedocs.io/en/latest/\n",
    "\n",
    "> `pip install -U deep_translator`\n",
    "\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# dictionary of supported languages\n",
    "GoogleTranslator().get_supported_languages(as_dict=True) \n",
    "\n",
    "import keys\n",
    "\n",
    "from deep_translator import single_detection\n",
    "\n",
    "**Sign up for a free account and get a free API key from: https://detectlanguage.com/documentation**-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [ISO-639-1 language codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)\n",
    "* [Google Translate’s supported languages](https://cloud.google.com/translate/docs/languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!--\n",
    "<p style=\"color:darkred; font-weight:bold;\">NOTE: There is currently a known issue with the translation in TextBlob<br/>Google changed the parameters to their webservice. The developer is aware of it and someone has submitted a fix, but it has not yet been merged into the repository. This should be fixed in a future version. I was able to get the basics working by modifying TextBlob's translate.py file, replacing:</p>\n",
    "\n",
    "> url = \"http://translate.google.com/translate_a/t?client=webapp&dt=bd&dt=ex&dt=ld&dt=md&dt=qca&dt=rw&dt=rm&dt=ss&dt=t&dt=at&ie=UTF-8&oe=UTF-8&otf=2&ssel=0&tsel=0&kc=1\"\n",
    "\n",
    "with \n",
    "\n",
    ">url = \"http://translate.google.com/translate_a/t?client=te&format=html&dt=bd&dt=ex&dt=ld&dt=md&dt=qca&dt=rw&dt=rm&dt=ss&dt=t&dt=at&ie=UTF-8&oe=UTF-8&otf=2&ssel=0&tsel=0&kc=1\"\n",
    "\n",
    "I also had to make a couple of other minor changes in the code snippets below.\n",
    "\n",
    "\n",
    "This article lists several packages that support language translation: https://dev.to/kalebu/how-to-do-language-translation-in-python-1ic6\n",
    "\n",
    "\n",
    "blob\n",
    "\n",
    "blob.detect_language()  # uses Google Translate; 'en' means English\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.7 Language Detection and Translation (2 of 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# autodetect source language and translate to Spanish\n",
    "spanish = translator.translate_text(blob.string, target_lang='es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spanish.detected_source_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spanish.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# autodetect source language and translate to Chinese\n",
    "chinese = translator.translate_text(blob.string, target_lang='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chinese.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.7 Language Detection and Translation (3 of 3)\n",
    "* Notice **differences** in the **text translated back to English** from Spanish and Chinese "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# autodetect source language and translate to English\n",
    "result = translator.translate_text(spanish.text, target_lang='en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.detected_source_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# autodetect source language and translate to English\n",
    "result = translator.translate_text(chinese.text, target_lang='en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.detected_source_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.9 Spell Checking and Correction (1 of 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word = Word('theyr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%precision 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word.spellcheck()  # returns tuples of corrections and confidence values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.9 Spell Checking and Correction (2 of 2)\n",
    "* `TextBlob`s, `Sentence`s and `Word`s all have a **`correct` method** \n",
    "* **Corrects spelling** using correctly spelled word with **highest confidence value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word.correct()  # chooses word with the highest confidence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence = TextBlob('Ths sentense has missplled wrds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence.correct() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.11 Word Frequencies via `word_counts` Dictionary in a `TextBlob` (1 of 2)\n",
    "* **Project Gutenberg's [60,000+ free e-books](https://www.gutenberg.org)**\n",
    "    * Great source of text corpora for analysis\n",
    "    * Read their [Terms of Use](https://www.gutenberg.org/wiki/Gutenberg:Terms_of_Use)\n",
    "    * **Out of copyright** in the **United States** \n",
    "* We **downloaded** the **Plain Text UTF-8** version of [Shakespeare’s *Romeo and Juliet*](https://www.gutenberg.org/ebooks/1513) \n",
    "    * Saved as **`RomeoAndJuliet.txt`** \n",
    "    * **Note**: For analysis, we **removed** the **Project Gutenberg text** before and after the play in each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.11 Word Frequencies via `word_counts` Dictionary in a `TextBlob` (2 of 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(Path('RomeoAndJuliet.txt').read_text())  # load Romeo and Juliet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Which word appears more in the play&mdash;\"Romeo\" or \"Juliet\"?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob.word_counts['juliet'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob.word_counts['romeo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.13 Deleting Stop Words (1 of 2)\n",
    "* Less significant words&mdash;like \"a\", \"an\", \"the\", pronouns, etc.&mdash;that are often removed before text analysis\n",
    "* Returned by [NLTK **`stopwords` module's `words` function**](https://www.nltk.org/book/ch02.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')  # must download before first use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.13 Deleting Stop Words (2 of 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')  # load the english list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = TextBlob('Today is a beautiful day.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keep anything that's not a stop word\n",
    "[word for word in blob.words.lower() if word not in stops]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[List comprehensions presentation in my **Python Fundamentals videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson05_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.3 Visualizing Word Frequencies with Bar Charts and Word Clouds (1 of 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Could not find an old-English stop words list\n",
    "* So I asked GenAI \n",
    "> **GenAI prompt:** Create a Python list of old english stop words from Romeo and Juliet. Include the items that would appear after apostrophes so words like s and t will be removed during stop word elimination. Do not include words that are already in the NLTK English stop words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops += [\n",
    "    'thou', 'thee', 'thy', 'thine', 'art', 'hast', 'hath', 'doth', 'dost',\n",
    "    'wilt', 'shalt', 'ye', 'ere', 'oft', 'naught', 'nay', 'anon', 'tarry', \n",
    "    'wot', 'whence', 'hence', 'whither', 'prithee', 'sirrah', 'zounds', \n",
    "    'forsooth', 'verily', 'fie', 'marry', 'troth', 'wherefore', 'hark', \n",
    "    'hither', 'thither', 'yon', 'yonder', 'thence', \n",
    "    \"'tis\", \"'twas\", \"'twere\", \"'twill\", \"'twould\", \"'d\", \"'s\", \"'ll\", \"'re\", \n",
    "    \"'ve\", \"'m\", \"'t\", \"o'\", \"'n\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(Path('RomeoAndJuliet.txt').read_text())  # load play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eliminate stopwords\n",
    "* `item[0]` is the **word in each tuple** returned by `blob.word_counts.items()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = blob.word_counts.items()  # iterator for word-frequency tuples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = [item for item in items if item[0] not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting the Top 20 Words in Descending Order by Frequency (2 of 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter  # used to specify tuple element to sort by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_items = sorted(items, key=itemgetter(1), reverse=True)  # descending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`key=itemgetter(1)`**&mdash;sort tuples by **frequency** (each tuple's element `1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top20 = sorted_items[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert top20 to a `DataFrame` for Visualization (3 of 4)\n",
    "* **pandas library** used frequently in later case studies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(top20, columns=['word', 'count'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the `DataFrame` (4 of 4)\n",
    "* **`bar` method** of the `DataFrame`’s **`plot` property** creates and displays a **Matplotlib bar chart**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "axes = df.plot.bar(x='word', y='count')\n",
    "plt.gcf().tight_layout()  # compress chart to ensure all components fit \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.2 Visualizing the Top 200 Words in  **Romeo and Juliet** as a Word Cloud (1 of 4)\n",
    "* `conda install -c conda-forge wordcloud`\n",
    "* Created by **Andreas Mueller**&mdash;author of [**\"Introduction to Machine Learning with Python\"**](https://amzn.to/2JTBKOp) and core developer of **scikit-learn machine-learning library**\n",
    "    \n",
    "### Loading the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = Path('RomeoAndJuliet.txt').read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Mask Image that Specifies the Word Cloud’s Shape (2 of 4)\n",
    "* [**`wordcloud` module’s**](https://github.com/amueller/word_cloud) **`WordCloud` class** uses **matplotlib** under the hood \n",
    "* Fills non-white areas of a **mask image** with text\n",
    "* Load the mask using **`imageio` module's `imread` function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import imageio  # bundled with Anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_image = imageio.v3.imread('mask_heart.png')  # returns NumPy array of image's data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NumPy discussed in Lesson 7, Array-Oriented Programming, of my **Python Fundamentals LiveLessons** videos](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson07_00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the WordCloud Object (3 of 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "    colormap='prism', mask=mask_image, background_color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `WordCloud` assigns **random colors** from a **color map**\n",
    "* [Matplotlib’s named color maps](https://matplotlib.org/examples/color/colormaps_reference.html)\n",
    "* [`WordCloud`’s keyword arguments and their default values](http://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Word Cloud, Saving It and Displaying It (4 of 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wordcloud = wordcloud.generate(text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removes stop words\n",
    "* Calculates the word frequencies\n",
    "* Uses up to **200 words** by default\n",
    "    * **`max_words` keyword argument** can specify any number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wordcloud = wordcloud.to_file('RomeoAndJulietHeart.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='RomeoAndJulietHeart.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.5 Named Entity Recognition with [**spaCy**](https://spacy.io/) (1 of 4)\n",
    "* Attempts to **locate and categorize items** that can help **determine what a text is about**\n",
    "    * **dates**, **times**, **quantities**, **places**, **people**, **things**, **organizations** and more \n",
    "* [spaCy Quickstart guide](https://spacy.io/usage/models#section-quickstart)\n",
    "* `conda install -c conda-forge spacy`\n",
    "* Download spaCy's **English (`en_core_web_sm`) model** for processing text  \n",
    ">```\n",
    ">ipython -m spacy download en_core_web_sm\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Language Model with the `spacy` Module’s **`load` Function** (2 of 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')  # loads English language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* spaCy docs recommend the **variable name `nlp`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a spaCy Doc (3 of 4)\n",
    "* Use the **`nlp` object** to create a [**spaCy `Doc`** object](https://spacy.io/api/doc) representing the **document** to process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document = nlp(\n",
    "    'In 1994, Tim Berners-Lee founded the World Wide Web Consortium which is devoted to developing web technologies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting the Named Entities Via a `Doc`’s **`ents` Property** (4 of 4)\n",
    "* Returns tuple of spaCy **`Span`** objects representing the **named entities** \n",
    "* [**`Span`** properties](https://spacy.io/api/span)\n",
    "* Display **`text`** (the **entity's text**) and **`label_`** (the **kind of entity**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for entity in document.ents:\n",
    "    print(f'{entity.text}: {entity.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo with dates and names never tried before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document = nlp(\n",
    "    \"\"\"In 2026, Amanda Brown will create a  \n",
    "    new company named AmandAI to help \n",
    "    you learn about Google's Gemini AI capabilities\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for entity in document.ents:\n",
    "    print(f'{entity.text}: {entity.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.7 Other NLP Libraries and Tools \n",
    "[See this section on O'Reilly](https://learning.oreilly.com/library/view/Python+for+Programmers,+First+Edition/9780135231364/ch11.xhtml#ch11lev1sec7)\n",
    "\n",
    "<!--\n",
    "Additional mostly free and open source NLP libraries and APIs: \n",
    "* **Gensim**—**Similarity detection** and **topic modeling**\n",
    "* **Google Cloud Natural Language API**—Cloud-based API for NLP tasks such as **named entity recognition**, **sentiment analysis**, **parts-of-speech analysis and visualization**, **determining content categories** and more\n",
    "* **Microsoft Linguistic Analysis API**\n",
    "* **Bing sentiment analysis**—**Microsoft’s Bing search engine** now uses **sentiment** in its **search results**\n",
    "* **PyTorch NLP**—**Deep learning library** for **NLP**  \n",
    "* **Stanford CoreNLP**—A **Java NLP library**, which also provides a **Python wrapper**. Includes **corefererence resolution**, which finds all references to the same thing.\n",
    "* **Apache OpenNLP**—Another **Java-based NLP library** for common tasks, including **coreference resolution**. **Python wrappers** are available.\n",
    "* **PyNLPl** (pineapple)—**Python NLP library** \n",
    "* **SnowNLP**—**Python library** that simplifies **Chinese text processing**\n",
    "* **KoNLPy**—**Korean language NLP**\n",
    "* **`stop-words`**—**Python library** with **stop words for many languages**. We used NLTK’s stop words lists in this chapter. \n",
    "* **`TextRazor`**—A **paid cloud-based NLP API** that provides a **free tier**\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.9 Natural Language Datasets \n",
    "[See this section on O'Reilly](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/ch11.xhtml#ch11lev1sec9)\n",
    "\n",
    "<!--\n",
    "* **Social media posts**&mdash;via APIs like the Twitter API we'll demonstrate next.\n",
    "* **Wikipedia**—some or all of Wikipedia (`https://meta.wikimedia.org/wiki/Datasets`).\n",
    "* **IMDB (Internet Movie Database)**—various **movie and TV datasets** are available.\n",
    "* **UCIs text datasets**—many datasets, including the **Spambase** dataset.\n",
    "* **Jeopardy! dataset**—200,000+ questions from the Jeopardy! TV show. A milestone in AI occurred in 2011 when IBM Watson famously beat two of the world’s best Jeopardy! players.\n",
    "* [**Natural language processing datasets**](https://machinelearningmastery.com/datasets-natural-language-processing/)\n",
    "* [**NLTK data**](https://www.nltk.org/data.html)\n",
    "* **Sentiment labeled sentences data set** (from sources including **IMDB.com**, **amazon.com**, **yelp.com**) \n",
    "* [**Registry of Open Data on AWS**](https://registry.opendata.aws)—a searchable directory of **datasets hosted on Amazon Web Services**.\n",
    "* [**Amazon Customer Reviews Dataset**](https://registry.opendata.aws/amazon-reviews/)—130+ million product reviews.\n",
    "* and many more!-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Info \n",
    "* See Lesson 11 in [**Python Fundamentals LiveLessons** here on O'Reilly Online Learning](https://learning.oreilly.com/videos/python-fundamentals/9780135917411)\n",
    "* See Chapter 11 in [**Python for Programmers** on O'Reilly Online Learning](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/)\n",
    "* See Chapter 12 in [**Intro Python for Computer Science and Data Science** on O'Reilly Online Learning](https://learning.oreilly.com/library/view/intro-to-python/9780135404799/)\n",
    "* Interested in a print book? Check out:\n",
    "\n",
    "| Python for Programmers<br>(640-page professional book) | Intro to Python for Computer<br>Science and Data Science<br>(880-page college textbook)\n",
    "| :------ | :------\n",
    "| <a href=\"https://amzn.to/2VvdnxE\"><img alt=\"Python for Programmers cover\" src=\"../images/PyFPCover.png\" width=\"150\" border=\"1\"/></a> | <a href=\"https://amzn.to/2LiDCmt\"><img alt=\"Intro to Python for Computer Science and Data Science: Learning to Program with AI, Big Data and the Cloud\" src=\"../images/IntroToPythonCover.png\" width=\"159\" border=\"1\"></a>\n",
    "\n",
    ">Please **do not** purchase both books&mdash;_Python for Programmers_ is a subset of _Intro to Python for Computer Science and Data Science_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; Copyright 1992-2024 by Pearson Education, Inc. All Rights Reserved. The content in this notebook is based on the book [**Python for Programmers**](https://amzn.to/2VvdnxE)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
