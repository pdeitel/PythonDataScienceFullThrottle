{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2024 by Pearson Education, Inc. All Rights Reserved. The content in this notebook is based on the book [**Python for Programmers**](https://amzn.to/2VvdnxE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- CSS settings for this notbook -->\n",
    "<style>\n",
    "    h1 {color:#BB0000}\n",
    "    h2 {color:purple}\n",
    "    h3 {color:#0099ff}\n",
    "    hr {    \n",
    "        border: 0;\n",
    "        height: 3px;\n",
    "        background: #333;\n",
    "        background-image: linear-gradient(to right, #ccc, black, #ccc);\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Big Data: Hadoop, Spark, NoSQL and IoT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enable high-res images in notebook \n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.6 Spark\n",
    "* Use **PySpark** and **Spark functional-style programming** to summarize **word frequencies** in **Romeo and Juliet**\n",
    "* Use **PySpark** and **Spark Structured Streaming** to summarize streaming hashtags from Mastodon's federated live stream\n",
    "* **Hadoop** break tasks into pieces that do **lots of disk I/O across many computers**\n",
    "* **Spark** performs certain big-data tasks **in memory** for **better performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6.1 Spark Overview\n",
    "* In big data, **performance is crucial**\n",
    "* **Hadoop** is geared to **disk-based** batch processing\n",
    "* Many **big-data** applications **demand better performance** \n",
    "    * E.g., **fast streaming** applications requiring **real-time** or **near-real-time processing**  \n",
    "* Spark **in-memory** architecture “**has been used to sort 100 TB of data 3X faster than Hadoop MapReduce on 1/10th of the machines**”[\\[2\\]](https://spark.apache.org/faq.html) \n",
    "* Runs some workloads up to **100 times faster** than Hadoop [\\[3\\]](https://spark.apache.org/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- \n",
    "### Architecture and Components (1 of 2)\n",
    "* Spark uses **resilient distributed datasets (RDDs)** to process distributed data with **functional-style programming** \n",
    "* Hadoop uses **replication for fault tolerance** &mdash; adds **more disk-based overhead**\n",
    "* **RDDs** eliminate disk-based overhead by \n",
    "    * **remaining in memory** &mdash; use disk only if data **can't fit in memory**\n",
    "    * **not replicating data**\n",
    "* **Fault tolerance** &mdash; Spark **remembers steps** used to **create an RDD**\n",
    "    * If a **node fails**, Spark **rebuilds the RDD** [\\[1\\]](https://spark.apache.org/research.html) \n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "### Architecture and Components (2 of 2)\n",
    "* **Spark distributes operations** to a cluster’s nodes for **parallel execution**\n",
    "* **Spark streaming** enables you to **process data as it’s received**\n",
    "* **Spark `DataFrame`s** (similar to pandas `DataFrames`), enable you to **manipulate RDDs** as a **collection of named columns**\n",
    "* Can use **Spark `DataFrame`s** with **Spark SQL** to **query distributed data**\n",
    "* **Spark MLlib** (the **Spark Machine Learning Library**) enables you to perform **machine-learning algorithms** on distributed data\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [**Databricks**](https://databricks.com)\n",
    "* A **Spark-specific vendor**  \n",
    "* Their website is an excellent resource for **learning Spark**\n",
    "* **Paid version** runs on **Amazon AWS** or **Microsoft Azure**\n",
    "* **Free Databricks Community Edition** is a great way to get started with both **Spark** and the **Databricks** environment\n",
    "* [**Databricks free e-books**](https://databricks.com/resources/type/ebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6.2 Docker and the Jupyter Docker Stacks\n",
    "### Docker \n",
    "* Some software packages we use in this chapter require **complicated setup and configuration**\n",
    "* **Preconfigured containers** can help you **get started** with new technologies **quickly and conveniently** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Docker Stacks \n",
    "<!--docker pull jupyter/pyspark-notebook:lab-3.4.7-->\n",
    "* Preconfigured [**Jupyter Docker stacks**](https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html) \n",
    "* **`jupyter/pyspark-notebook`** is configured with **Spark and PySpark** \n",
    "* For details on installing and running **Docker** and the **`jupyter/pyspark-notebook`**, see \n",
    "    * [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson16_34)\n",
    "    * [**Python for Programmers** Section 16.6.2](https://learning.oreilly.com/library/view/Python+for+Programmers,+First+Edition/9780135231364/ch16.xhtml#ch16lev2sec25)\n",
    "\n",
    "```\n",
    "docker run -p 8888:8888 -p 4040:4040 -it --user root \\\n",
    "    -v fullPathTo/ch16:/home/jovyan/work \\\n",
    "    jupyter/pyspark-notebook start.sh jupyter lab\n",
    "```\n",
    "<!-- 04/24/2023 docker run -p 8895:8888 -p 4040:4040 -it --user root -v ~/Documents/PythonDataScienceFullThrottleNEW/ch16:/home/jovyan/work jupyter/pyspark-notebook start.sh jupyter lab -->\n",
    "\n",
    "* **Install TextBlob and Mastodon into the container**\n",
    "    * `docker ps`  \n",
    "    **lists running containers**\n",
    "    * `docker exec -it container_name /bin/bash`  \n",
    "    **replace container_name with the name from `docker ps`**\n",
    "    * `conda install textblob`\n",
    "    * `pip install mastodon.py`\n",
    "    \n",
    "<!--\n",
    "docker run --rm -p 8895:8888 -p 4040:4040 -it --user root -v .:/home/jovyan/work deitelpydsft start.sh jupyter lab\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `deitelpydsft` Docker Container\n",
    "If you built the Docker container for my course using the Dockerfile in the GitHub repo (which already includes the software above), you can run\n",
    "\n",
    ">`docker run --rm -p 8895:8888 -p 4040:4040 -it --user root -v .:/home/jovyan/work deitelpydsft start.sh jupyter lab`\n",
    "\n",
    "I used port **8895**, since **8888** is where I am running JupyterLab today.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6.3 Word Count with Spark\n",
    "* Use Spark **filter, map and reduce** to implement a simple **word count** example that summarizes the words in **Romeo and Juliet**\n",
    "\n",
    "### Loading the NLTK Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a SparkContext \n",
    "* A **`SparkContext`** object gives you access to Spark’s capabilities\n",
    "* Some Spark environments **create a `SparkContext` for you** but not the Jupyter Docker stack\n",
    "* To create a **`SparkContext`**\n",
    "    * Specify the **configuration options** with a **`SparkConf`** object \n",
    "    * **`setMaster`** specifies the **Spark cluster’s URL**\n",
    "    * **`local[*]`** &mdash; Spark is executing on your **`local` computer** \n",
    "    * **`*`** &mdash; Use same number of **threads** as **cores** on your computer\n",
    "        * Simulates **parallelism of Spark clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "configuration = SparkConf().setAppName('RomeoAndJulietCounter')\\\n",
    "                           .setMaster('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(conf=configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Text File and Mapping It to Words\n",
    "* You work with a **`SparkContext`** using **functional-style programming** applied to an **RDD**\n",
    "* **RDD** enables you to **transform the data** stored throughout a **cluster** in **HDFS**\n",
    "* Get a new **`RDD`** representing all words in **Romeo and Juliet**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textblob.utils import strip_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized = sc.textFile('RomeoAndJuliet.txt')\\\n",
    "              .flatMap(lambda line: line.lower().split())\\\n",
    "              .map(lambda word: strip_punc(word, all=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the Stop Words\n",
    "* Get a new **`RDD`** with **no stop words** remaining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered = tokenized.filter(lambda word: word not in stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Each Remaining Word \n",
    "* Now we can **count** the number of **occurrences** of each word\n",
    "* First **`map`** each word to a **tuple** containing the **word** and **`1`**\n",
    "* **`reduceByKey`** with the **`operator`** module’s **`add` function** as an argument **adds** the counts for tuples that contain same **key** (`word`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "word_counts = filtered.map(lambda word: (word, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping Only the Words with Counts Greater Than or Equal to 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_counts = word_counts.filter(lambda item: item[1] >= 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting in Descending Order and Displaying the Results (1 of 2)\n",
    "* At this point, we’ve specified all the steps to **count the words**\n",
    "* When you call an **`RDD`'s `collect` method**, **Spark** \n",
    "    * initiates the **processing steps**\n",
    "    * **returns a list** containing the final results &mdash; **word-count tuples**\n",
    "* Everything **appears to execute on one computer**\n",
    "* Spark **distributes tasks among the cluster’s worker nodes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sorted_items = sorted(filtered_counts.collect(),\n",
    "                      key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting and Displaying the Results (2 of 2)\n",
    "* We determine the **word with the most letters** so we can **right-align** the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   romeo: 298\n",
      "    thou: 277\n",
      "  juliet: 178\n",
      "     thy: 170\n",
      "   nurse: 146\n",
      " capulet: 141\n",
      "    love: 136\n",
      "    thee: 135\n",
      "   shall: 110\n",
      "    lady: 109\n",
      "   friar: 104\n",
      "    come: 94\n",
      "mercutio: 83\n",
      "    good: 80\n",
      "benvolio: 79\n",
      "   enter: 75\n",
      "      go: 75\n",
      "    i’ll: 71\n",
      "  tybalt: 69\n",
      "   death: 69\n",
      "   night: 68\n",
      "lawrence: 67\n",
      "     man: 65\n",
      "    hath: 64\n",
      "     one: 60\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(word) for word, count in sorted_items])\n",
    "for word, count in sorted_items:\n",
    "    print(f'{word:>{max_len}}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# terminate current SparkContext so we can create another for next example\n",
    "sc.stop()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Monitoring Interface\n",
    "* https://spark.apache.org/docs/latest/monitoring.html\n",
    "* http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6.4 Spark Word Count on Microsoft Azure\n",
    "* For instructions on how to **run the preceding Spark example using Microsoft Azure HDInsight** see \n",
    "    * [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/9780135917411-PFLL_Lesson16_36) \n",
    "    * [**Python for Programmers, Section 16.6.4**](https://learning.oreilly.com/library/view/Python+for+Programmers,+First+Edition/9780135231364/ch16.xhtml#ch16lev2sec27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.7 Spark Structured Streaming: Counting Hashtags Using the `pyspark-notebook` Docker Stack\n",
    "* Spark Streaming vs. Structured Streaming:  \n",
    "https://dzone.com/articles/spark-streaming-vs-structured-streaming\n",
    "* Spark: RDD vs DataFrames:  \n",
    "https://blog.knoldus.com/spark-rdd-vs-dataframes/\n",
    "* Structured Streaming Programming Guide:  \n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n",
    "### Application Overview\n",
    "* **Stream toots** and summarize **top-20 hashtags** in the live federated stream \n",
    "* Display in dynamically updating **bar chart** \n",
    "* **Spark Structured Streaming** will read toots and **summarize hashtags**\n",
    "* Parts of this sample app communicate with one another via **network sockets**\n",
    "    * low-level **client/server networking** in which a **client** app communicates with a **server** app over a **network** using techniques **similar to file I/O**\n",
    "    * **socket** represents one **endpoint of a connection**\n",
    "* We installed **Mastodon.py** into the `pyspark-notebook` Docker stack (described earlier in the notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.7.1 Streaming toots to a Socket\n",
    "* The script **`starttootstream.py`** contains modified **`TootListener` class** from our Data Mining Mastodon presentation\n",
    "* **Streams** the specified number of toots and **sends them to a socket on the local computer**\n",
    "* Requires **`keys_mastodon.py`** containing **Mastodon credentials**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ipython starttootstream.py 1000### Executing the Script in the Docker Container\n",
    "* **File > New > Terminal** \n",
    "* `cd work`\n",
    "* `ipython starttootstream.py number_of_toots`\n",
    ">```\n",
    ">ipython starttootstream.py 1000\n",
    ">```\n",
    "* Script displays `\"Waiting for connection\"` until Spark connects to begin streaming the toots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class TOOTListener (see `starttootstream.py`)\n",
    "* **Method `on_update`** extracts hashtags, converts to lowercase and creates a space-separated string of hashtags to send to Spark \n",
    "* Uses `connection`’s **`send`** method to send string to whatever is reading from that socket \n",
    "    * **`send`** expects as its argument a **sequence of bytes**\n",
    "    * **`hashtags_string.encode('utf-8')`** converts a string to bytes \n",
    "* Spark Structured Streaming automatically reconstructs the strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main Application \n",
    "* Get the **number of toots to process** \n",
    "* Get **socket object** that we’ll use to wait for a connection from the Spark application\n",
    "* **Bind the socket** to a hostname or IP address and a port number \n",
    "    * Script **listens** for an initial connection  \n",
    "* **Wait until a connection is received** before starting the stream\n",
    "* **`accept`** the connection\n",
    "    * Returns a tuple containing a new **socket object** that the script will use to communicate with the Spark application and the **IP address** of the Spark application’s computer\n",
    "* Start the Mastodon federatedfederated stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 16.7.2 Summarizing Toot Hashtags\n",
    "* Use **Spark Structured Streaming** to read the hashtags sent via a socket by the script `starttootstream.py` and summarize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pyspark.sql.functions import explode, split\n",
    "from pyspark.sql import SparkSession\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function to Display a Barplot of the Top-20 Hashtags So Far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_barplot(epoch_id, spark_df, x, y, scale=1.0, size=(8, 4.5)):\n",
    "    \"\"\"Displays a Spark DataFrame's contents as a bar plot.\"\"\"\n",
    "    df = spark_df.toPandas()\n",
    "    \n",
    "    # remove prior graph when new one is ready to display\n",
    "    clear_output(wait=True) \n",
    "\n",
    "    if not df.empty: \n",
    "        print(f'EPOCH_ID: {epoch_id}')\n",
    "        df = df.sort_values(by='count', ascending=False).head(20)\n",
    "    else:\n",
    "        print(f'EPOCH_ID: {epoch_id}. Empty DataFrame')\n",
    "        return\n",
    "    \n",
    "    # create and configure a Figure containing a Seaborn barplot \n",
    "    plt.figure(figsize=size)\n",
    "    sns.set(font_scale=scale)\n",
    "    barplot = sns.barplot(data=df, x=x, y=y, \n",
    "                          palette=sns.color_palette('cool', 20))\n",
    "    \n",
    "    # rotate the x-axis labels 90 degrees for readability\n",
    "    for item in barplot.get_xticklabels():\n",
    "        item.set_rotation(90)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Number of Cores Available to the App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cores = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a SparkSession to Track Hashtags\n",
    "* Runs on local machine using all available cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('HashtagFrequency') \\\n",
    "    .master('local') \\\n",
    "    .config('spark.driver.cores', cores) \\\n",
    "    .config('spark.executor.cores', cores) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a DataFrame to Read Streaming Data\n",
    "* Will read lines of space-separated hashtags from socket\n",
    "* Following statement connects to the streaming app using a **socket** on **localhost** at port **9876**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines = spark.readStream \\\n",
    "    .format('socket') \\\n",
    "    .option('host', 'localhost') \\\n",
    "    .option('port', 9876) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create New Spark DataFrame in Which Each Hashtag is a Separate Row \n",
    "* 1. `split()` call breaks line of text at each space\n",
    "* 2. `explode()` call creates new DataFrame row for each hashtag\n",
    "* 3. `select()` call applies its argument to each line of text to build the new DataFrame\n",
    "* 4. `alias()` call creates a table name for the DataFrame -- used in SQL queries across distributed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hashtags = lines.select(explode(split(lines.value, ' ')).alias('hashtag'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Count Frequencies of the Hashtags\n",
    "* Creates two-column result containing hashtags and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = hashtags.groupBy('hashtag').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### start the app and call display_barplot every 10 seconds\n",
    "1. `writeStream` returns a `DataStreamWriter` used to configure the app's output\n",
    "2. `outputMode()` call indicates **all results will be output**, not just new/updated ones\n",
    "3. `forEachBatch()` call receives the `DataFrame` and a unique epoch ID for the current microbatch of data being processed\n",
    "4. `trigger()` call sets amount of time between batches of data; can be in seconds, minutes, hours or a combo\n",
    "5. `start()` launches the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# disable glyph warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = counts.writeStream \\\n",
    "    .outputMode('complete') \\\n",
    "    .foreachBatch(lambda df, epoch_id: display_barplot(epoch_id, df, x='hashtag', y='count')) \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Wait for Streaming to Finish\n",
    "* You'd execute this in a standalone app\n",
    "* Can call `query.stop()` to terminate app "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query.awaitTermination(timeout=600) # terminate in 600 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Info \n",
    "* See Lesson 16 in [**Python Fundamentals LiveLessons** here on O'Reilly Online Learning](https://learning.oreilly.com/videos/python-fundamentals/9780135917411)\n",
    "* See Chapter 16 in [**Python for Programmers** on O'Reilly Online Learning](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/)\n",
    "* See Chapter 17 in [**Intro Python for Computer Science and Data Science** on O'Reilly Online Learning](https://learning.oreilly.com/library/view/intro-to-python/9780135404799/)\n",
    "* Interested in a print book? Check out:\n",
    "\n",
    "| Python for Programmers<br>(640-page professional book) | Intro to Python for Computer<br>Science and Data Science<br>(880-page college textbook)\n",
    "| :------ | :------\n",
    "| <a href=\"https://amzn.to/2VvdnxE\"><img alt=\"Python for Programmers cover\" src=\"../images/PyFPCover.png\" width=\"150\" border=\"1\"/></a> | <a href=\"https://amzn.to/2LiDCmt\"><img alt=\"Intro to Python for Computer Science and Data Science: Learning to Program with AI, Big Data and the Cloud\" src=\"../images/IntroToPythonCover.png\" width=\"159\" border=\"1\"></a>\n",
    "\n",
    ">Please **do not** purchase both books&mdash;_Python for Programmers_ is a subset of _Intro to Python for Computer Science and Data Science_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 1992-2024 by Pearson Education, Inc. All Rights Reserved. The content in this notebook is based on the book [**Python for Programmers**](https://amzn.to/2VvdnxE)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
